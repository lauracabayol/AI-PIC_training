{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683870a6-98f8-4fc6-a22c-172fe8b6defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e929c3-3e71-48ab-b2d3-449fb39bbe59",
   "metadata": {},
   "source": [
    "# Introduction to Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9fd2a-2100-4878-8d3c-1767e705878c",
   "metadata": {},
   "source": [
    "In this notebook, we'll cover the basics of backpropagation and how it is implemented in PyTorch using the autograd package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ead2f7-fb12-4509-97a4-9cab04dafb21",
   "metadata": {},
   "source": [
    "## Part 1: What is Backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0798a10-4f8f-49cc-9782-77b3ce03ef08",
   "metadata": {},
   "source": [
    "Backpropagation is an algorithm used to train artificial neural networks by efficiently computing the gradients of the loss function with respect to the network's parameters. It is crucial in deep learning algorithms to enable the training of deep neural networks on large datasets.\n",
    "\n",
    "The basic idea behind backpropagation is to use the chain rule to compute the gradients of the loss function with respect to each trainable parameter in the network. This is done by working backwards through the network, starting from the output layer and moving towards the input layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5396305-43c5-4c9d-a436-d205717b1a26",
   "metadata": {},
   "source": [
    "\n",
    "The key steps of the backpropagation algorithm are:\n",
    "\n",
    "- Forward pass: compute the output of the network given an input.\n",
    "- Compute the loss: compute the difference between the network's output and the target output.\n",
    "- Backward pass: compute the gradients of the loss with respect to each parameter in the network using the chain rule.\n",
    "- Update the parameters: update each parameter in the network using an optimization algorithm (e.g., stochastic gradient descent) to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b426dae-c4ff-4811-a688-c176d926a688",
   "metadata": {},
   "source": [
    "### Mathematics of Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3e170-d9b3-47ac-ad42-848ced8a466c",
   "metadata": {},
   "source": [
    "Let's consider a simple feedforward neural network with one hidden layer:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42cba610-9884-497e-8290-789af492130d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "x_1      w_{1,1}     w_{1,2}    b_1\n",
    " o--------->o--------->o----->(+)----->o\n",
    "x_2      w_{2,1}     w_{2,2}    b_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ccb95-b6e0-45a3-8111-fdc0415eb051",
   "metadata": {},
   "source": [
    "where x_1 and x_2 are the input features, w_{i,j} are the weights connecting the i-th input to the j-th hidden neuron, b_i is the bias of the i-th hidden neuron, f is the activation function, and o represents the output of each neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c631c3d-ce35-42cf-8e9a-6c17a1b5f6b8",
   "metadata": {},
   "source": [
    "We assume that the output layer has a single neuron that produces the final prediction y."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e6b49ba-cc21-444c-913a-5f3b02c67b3f",
   "metadata": {},
   "source": [
    "o = f(w_{1,1}*x_1 + w_{1,2}*x_2 + b_1) * w_{2,1} + f(w_{1,1}*x_1 + w_{1,2}*x_2 + b_1) * w_{2,2} + b_2\n",
    "  = f(z_1) * w_{2,1} + f(z_2) * w_{2,2} + b_2\n",
    "  = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77e58a-2741-47ab-bac9-7a19a1513608",
   "metadata": {},
   "source": [
    "The pre-activation output is a linear combination of the hidden layer activations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "424e7f44-c606-4c8a-9ec0-a656c436a872",
   "metadata": {},
   "source": [
    "z_j = w_{1,j}*f(z_1) + w_{2,j}*f(z_2) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9e92e-dc26-4849-954a-230870e433a6",
   "metadata": {},
   "source": [
    "where z_1 and z_2 are defined as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4297bb5e-f9f3-437e-90e9-62e9754a55bf",
   "metadata": {},
   "source": [
    "z_1 = w_{1,1}*x_1 + w_{2,1}*x_2 + b_1\n",
    "z_2 = w_{1,2}*x_1 + w_{2,2}*x_2 + b_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18c5b0-a603-4989-92cf-b9a83f2cd6d5",
   "metadata": {},
   "source": [
    "where w_{i,j} represents the weight connecting the i-th input to the j-th neuron in the hidden layer, b_1 is the bias term for the hidden layer, and x_1 and x_2 are the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65646036-d020-4808-9f5b-0313fa6fecdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb394db7-5771-4d2f-bc31-2997d126fc43",
   "metadata": {},
   "source": [
    "Our goal is to train the network to minimize a given loss function L(y, y_true) with respect to the weights and biases of the network, where y_true is the ground-truth label. The backpropagation algorithm computes the gradients of the loss function with respect to each weight and bias using the chain rule:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41351572-7dcb-41a0-b7f9-6da153d5e59d",
   "metadata": {},
   "source": [
    "dL/dw_{i,j} = dL/dy * dy/dz_i * dz_i/dw_{i,j}\n",
    "dL/db_i = dL/dy * dy/dz_i * dz_i/db_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb162acf-0e33-4988-b0e7-22f502c1cb1f",
   "metadata": {},
   "source": [
    "where i and j range over the layers and neurons of the network, and dz_i/dw_{i,j} and dz_i/db_i are the partial derivatives of the pre-activation output z_i with respect to the weights and biases of the network, respectively.\n",
    "\n",
    "The gradients are computed in a backwards pass through the network, starting from the output layer and moving towards the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936dc103-da60-4c76-bd10-d4d871a132b5",
   "metadata": {},
   "source": [
    "For example, the gradients of the loss function with respect to the weights and biases of the output layer can be computed as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f4a0d58-178c-4901-94b2-12668c4893a4",
   "metadata": {},
   "source": [
    "dL/dw_{2,j} = dL/dy * dy/dw_{2,j} = dL/dy * f(z_j)\n",
    "dL/db_2 = dL/dy * dy/db_2 = dL/dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424ff46-f28c-4e6b-b330-d27708b55708",
   "metadata": {},
   "source": [
    "where j is the index of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6ee9c-4acc-4899-a84c-9aca778b2e00",
   "metadata": {},
   "source": [
    "##Â Part 2: Autograd in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0933e49-82e7-4e53-9d27-b4a02caa102a",
   "metadata": {},
   "source": [
    "PyTorch provides a powerful automatic differentiation package called autograd, which makes it easy to compute gradients of tensor-valued functions. Autograd works by keeping track of the operations performed on tensors and building a computational graph that represents the chain of operations. This graph is used to efficiently compute the gradients of the loss function with respect to each parameter in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b111814-0dac-47f7-98bf-5f45a86fb0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9315,  0.2164,  0.0336,  0.2572, -1.5692, -1.5824, -1.2028, -0.3714,\n",
       "         0.8283,  1.5677])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a tensor variable\n",
    "x = torch.randn(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa91ff5-cbc6-47bd-824f-ecae6d49262a",
   "metadata": {},
   "source": [
    "To use autograd, we need to define a tensor that tracks the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0329baa0-44f4-4d5f-ac90-2e4122c715d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3928, -0.8631, -0.2639, -1.6774, -0.4690,  0.4454,  0.3350,  0.4344,\n",
       "        -0.1703, -0.9364], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d87c18-1394-4e5a-8206-d9c7a653b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function using the tensor variable\n",
    "y = (x**2 + 2*x + 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008cf5e-19f5-4802-aac8-f7c3bd27ac15",
   "metadata": {},
   "source": [
    "To compute the gradients in pytorch, we need to use the function 'backward()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee25c9d-ebfc-4b13-a0d4-294c47f77bdb",
   "metadata": {},
   "source": [
    "The backward() function can only run on scalar outputs. We need to take the mean in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e485dc4-31a7-49e8-aa53-c8809065017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradients using autograd\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a599df-b8e4-4250-9aca-8af528c2a21d",
   "metadata": {},
   "source": [
    "The variable with gradients is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9adb95f-cf3b-4fe8-a37d-8144404b77f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2786,  0.0274,  0.1472, -0.1355,  0.1062,  0.2891,  0.2670,  0.2869,\n",
      "         0.1659,  0.0127])\n"
     ]
    }
   ],
   "source": [
    "# print the gradients\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988cc68-661b-47b3-b6be-be85a0e49857",
   "metadata": {},
   "source": [
    "This tells us that the gradient of the function y = x^2 + 2x + 1 with respect to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05896b2-7104-417e-8af2-961e7f04c6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv2",
   "language": "python",
   "name": "dlenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
